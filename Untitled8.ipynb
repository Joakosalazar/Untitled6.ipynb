{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13KU4aFLtiqOa2C8omHUqGSknYZC0J944",
      "authorship_tag": "ABX9TyPITUkeZ0Z86SBMU2gYqIIj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joakosalazar/Untitled6.ipynb/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Pasamos el archivo zip para convertirlo\n",
        "zip_path = './skin_cancer (3).zip'\n",
        "\n",
        "# Extraemos el contenido\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./skin_cancer (3)')\n"
      ],
      "metadata": {
        "id": "yTbaHLvGEucU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Extraido del dataset\n",
        "extracted_path = './skin_cancer (3)/skin_cancer/skin_cancer/data'\n",
        "\n",
        "# Directorio de contenidos\n",
        "for root, dirs, files in os.walk(extracted_path):\n",
        "    print(root, len(dirs), len(files))\n"
      ],
      "metadata": {
        "id": "lx5glL3vF4Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definimos los datasets\n",
        "class SkinCancerDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, subdir in enumerate(['benign', 'malignant']):\n",
        "            subdir_path = os.path.join(image_dir, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                for img_name in os.listdir(subdir_path):\n",
        "                    if img_name.endswith(('png', 'jpg', 'jpeg')):  # Ensure only image files are added\n",
        "                        self.image_files.append(os.path.join(subdir_path, img_name))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Definimos las transformaciones\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "OHSfcDE_FNwV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del dataset\n",
        "ruta_train = '/content/skin_cancer/Skin/data/train/train-20240324T151905Z-001/train'\n",
        "ruta_test = '/content/skin_cancer/Skin/data/test/test-20240324T151902Z-001/test'\n",
        "  # Verify this path\n",
        "dataset = SkinCancerDataset(image_dir=image_dir, transform=data_transforms)\n",
        "train_dataset = datasets.ImageFolder(ruta_train, transform=data_transforms)\n",
        "test_dataset = datasets.ImageFolder(ruta_test, transform=data_transforms)\n"
      ],
      "metadata": {
        "id": "J7CI_DMmFTvl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimimos para comprobar el numero de imagenes que se encuentran. Por algun motivo no encuentra ninguna.\n",
        "print(f\"Number of images found: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VYv8m3p-HYoR",
        "outputId": "50a32a96-205b-459c-daf1-2f2dfb4ef987"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images found: 660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el modelo pre entrenado ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n"
      ],
      "metadata": {
        "id": "nHq5KKS1GmlO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "67f299f1-8295-45e1-9e13-91001da76684"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos funcion de perdida y optimizacion del modelo\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "9tG82uLXGrfg"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkinCancerDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        print(\"Image directory:\", self.image_dir)  # Print to verify the path\n",
        "\n",
        "        for label, subdir in enumerate(['benign', 'malignant']):\n",
        "            subdir_path = os.path.join(image_dir, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                for img_name in os.listdir(subdir_path):\n",
        "                    if img_name.endswith(('png', 'jpg', 'jpeg')):\n",
        "                        self.image_files.append(os.path.join(subdir_path, img_name))\n",
        "                        self.labels.append(label)\n",
        "            else:\n",
        "                print(f\"Warning: Subdirectory {subdir_path} not found.\")\n",
        "\n",
        "        if len(self.image_files) == 0:\n",
        "            print(\"Error: No image files found in the directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QFVP8GgtZlE-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la carga de datos\n",
        "traindataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "testdataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "yUXlWeeNHJKc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos\n",
        "num_epochs = 1\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in traindataloader : # Iteramos en la carga de datos\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dmLycQe5GtuB",
        "outputId": "971aa9a0-d412-4a20-d30d-05703f5845d6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0, Loss: 0.4528, Accuracy: 0.7910\n"
          ]
        }
      ]
    }
  ]
}